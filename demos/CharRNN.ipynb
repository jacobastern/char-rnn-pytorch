{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character RNN\n",
    "#### Description:\n",
    "We implement a character-level recurrent neural network using the GRU as our recurrent unit. We train the CharRNN on Hamilton lyrics, and get some interesting results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uN0jhJnyOiWh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gj3UWCw2J6q"
   },
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 749,
     "status": "error",
     "timestamp": 1540322700079,
     "user": {
      "displayName": "Jacob Stern",
      "photoUrl": "",
      "userId": "14292350991336586550"
     },
     "user_tz": 360
    },
    "id": "RQS-dGYoKXQw",
    "outputId": "b7dace2f-fc85-4666-aefe-cbfc06e2b047"
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, chunk_len=200, file_name=\"michael-jackson.txt\"):\n",
    "        \"\"\"Creates a Pytorch Dataset from a text corpus.\n",
    "        Args:\n",
    "            chunk_len (int): the length of each training segment of text\n",
    "            file_name (str): the name of the file to train on. A full list of files is found here: https://www.kaggle.com/paultimothymooney/poetry\n",
    "        \"\"\"\n",
    "        \n",
    "        root = '../data/'\n",
    "            \n",
    "        text_files = os.listdir(root)\n",
    "        self.training_file = text_files[text_files.index(file_name)]\n",
    "        with open(os.path.join(root, self.training_file), encoding='utf-8') as file:\n",
    "            self.training_file = file.read()\n",
    "        self.segment_extractor = self.FileSegmentExtractor(self.training_file, chunk_len)\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_zip(zip_path):\n",
    "        \"\"\"Extraxts a file in .zip format to the root directory\n",
    "        Args:\n",
    "            zip_path (str): the path to the zip file\n",
    "        \"\"\"\n",
    "        print('Unzipping {}'.format(zip_path))\n",
    "        with zipfile.ZipFile(zip_path,\"r\") as zip_ref:\n",
    "            zip_ref.extractall(os.path.dirname(self.root))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        \n",
    "    class FileSegmentExtractor():\n",
    "        def __init__(self, training_file, chunk_len):\n",
    "            \"\"\"Extracts chunk_len segments from the data for training\n",
    "            Args:\n",
    "                training_file (str): the training file path\n",
    "                chunk_len (int): the number of characters in each chunk\n",
    "            \"\"\"\n",
    "            self.chunk_len = chunk_len\n",
    "            self.training_file = training_file\n",
    "            self.file_len = len(self.training_file)\n",
    "            # A string including all printable characters\n",
    "            self.all_characters = string.printable\n",
    "            self.n_characters = len(self.all_characters)\n",
    "            \n",
    "        def random_chunk(self):\n",
    "            \"\"\"Extracts a random chunk from the file\n",
    "            Returns:\n",
    "                (str): a string of length (chunk_len)\n",
    "            \"\"\"\n",
    "            start_index = random.randint(0, self.file_len - self.chunk_len)\n",
    "            end_index = start_index + self.chunk_len + 1\n",
    "            return self.training_file[start_index:end_index]\n",
    "\n",
    "        def char_tensor(self, string):\n",
    "            \"\"\"Converts characters in a string to a numerical index representing that character.\n",
    "            Args:\n",
    "                string (str): the string to convert\n",
    "            Returns:\n",
    "                tensor (torch.Tensor): a tensor containing the indices of each letter in the string\n",
    "            \"\"\"\n",
    "            tensor = torch.zeros(len(string)).long()\n",
    "            for c in range(len(string)):\n",
    "                try:\n",
    "                    tensor[c] = self.all_characters.index(string[c])\n",
    "                except ValueError:\n",
    "                    tensor[c] = self.all_characters.index(' ')\n",
    "            return tensor\n",
    "\n",
    "        \n",
    "        def random_training_set(self):\n",
    "            \"\"\"Obtains a random set of data to train on.\n",
    "            Returns:\n",
    "                inp (torch.Tensor): a chunk of characters from the file\n",
    "                target (torch.Tensor): the same chunk of characters offset by one\n",
    "            \"\"\"\n",
    "            chunk = self.random_chunk()\n",
    "            inp = self.char_tensor(chunk[:-1])\n",
    "            target = self.char_tensor(chunk[1:])\n",
    "            return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-0hC1GV14Gy"
   },
   "source": [
    "# Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1128,
     "status": "ok",
     "timestamp": 1539659284932,
     "user": {
      "displayName": "Jacob Stern",
      "photoUrl": "",
      "userId": "14292350991336586550"
     },
     "user_tz": 360
    },
    "id": "gI4ppqOe12_1",
    "outputId": "548fd66d-7119-497c-fdae-744cff23bade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hrowing away my shot\n",
      "Hey yo, I'm just like my country\n",
      "I'm young, scrappy and hungry\n",
      "And I'm not throwing away my shot We're gonna rise up (time to take a shot)\n",
      "We're gonna rise up (time to take a shot)\n"
     ]
    }
   ],
   "source": [
    "print(TextDataset(file_name='lin-manuel-miranda.txt').segment_extractor.random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyYLpLrZ2Q0e"
   },
   "source": [
    "# GRU Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hUeB1dTM19hR"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"A gated recurrent unit, described here: https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
    "        Args:\n",
    "            input_size (int): the dimension of the input of the GRU.\n",
    "            hidden_size (int): the dimension of the hidden state outputs of the GRU. \n",
    "        \"\"\"\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.wr = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.wz = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.w = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, x_input, prev_hidden):\n",
    "        \"\"\"A forward pass of an input through the model. Follows the 'fully-gated' GRU architecture\n",
    "        Args:\n",
    "            x_input ((self.input_size) torch.Tensor): an integer-encoded token\n",
    "            prev_hidden ((self.hidden_size) torch.Tensor): the hidden layer output of the previous GRU\n",
    "        Returns:\n",
    "            ht ((self.hidden_size) torch.Tensor): the resulting hidden layer representation\n",
    "            ht ((self.hidden_size) torch.Tensor): another copy of the hidden layer, which will go\n",
    "                through one more transformation to become the output token prediction.\n",
    "        \"\"\"\n",
    "        zt = self.sig(self.wz(torch.cat((x_input, prev_hidden), dim=2)))\n",
    "        rt = self.sig(self.wr(torch.cat((x_input, prev_hidden), dim=2)))\n",
    "        h_tilde = self.tanh(self.w(torch.cat((torch.mul(rt, prev_hidden), x_input), dim=2)))\n",
    "        ht = torch.mul((1 - zt), prev_hidden) + torch.mul(zt, h_tilde)\n",
    "        return ht, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVOONUQJKW3j"
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8K3iqVdTT5U"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        \"\"\"A full recurrent neural network, built around a GRU.\n",
    "        Args:\n",
    "            input_size (int): the dimension of the input to the network - in the \n",
    "                case of our CharRNN, it is len(alphabet). \n",
    "            hidden_size (int): the dimension of the hidden state outputs of the GRU. \n",
    "            output_size (int): the dimension of the output of the network. Note that \n",
    "                the output size is the same as the input size.\n",
    "            n_layers (int): the number of layers in the RNN\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        # Input: An integer encoding of the character\n",
    "        self.input_size = input_size\n",
    "        # Output: A categorical distribution over characters\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # An embedding layer is different from a linear layer because it provides\n",
    "        # lookup capability -- each character has its own trained embedding. A\n",
    "        # linear layer is different, as all input characters share the same weights\n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # The first argument is self.hidden_size because we've embedded the input to be the same size as the hidden_state\n",
    "        self.gru = GRU(self.hidden_size, self.hidden_size)\n",
    "        self.to_output_size = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1) #dim=1\n",
    "\n",
    "    def forward(self, input_char, hidden_state):\n",
    "        \"\"\"A forward pass through the RNN.\n",
    "        Args:\n",
    "            input_char (int): an integer-encoded character\n",
    "            hidden_state ((self.hidden_size) torch.Tensor): the hidden state output of the previous layer\n",
    "        \"\"\"\n",
    "        embed = self.embedding(input_char).view(1,1,-1)\n",
    "        output, hidden = self.gru(embed, hidden_state)\n",
    "        output = self.relu(self.to_output_size(output))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evbm5yBq2Ycf"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wB0sd9AGK9n3"
   },
   "outputs": [],
   "source": [
    "def step(input_string, target_string, decoder, decoder_optimizer, criterion):\n",
    "    \"\"\"Takes one step of training\n",
    "    Args:\n",
    "        input_string (torch.Tensor): a string of characters each encoded to a unique integer\n",
    "            and stored in a tensor\n",
    "        target_string (torch.Tensor): a string of characters each encoded to a unique integer\n",
    "            and stored in a tensor. The same as the input string, but offset by one.\n",
    "        decoder (torch.nn.Module): the RNN model\n",
    "        decoder_optimizer (torch.optim.Optimizer): the optimizer for the model\n",
    "        criterion (torch.nn.modules.loss._Loss): the objective function\n",
    "    Returns:\n",
    "        (torch.Tensor): a zero-dimensional tensor holding the loss-per-character\n",
    "    \"\"\"\n",
    "    # initialize hidden layers, set up gradient and loss\n",
    "    loss = 0\n",
    "    hidden = decoder.init_hidden()\n",
    "    num_classes = len(string.printable)\n",
    "    i = 0\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    for in_char, target_char in zip(input_string, target_string):\n",
    "        \n",
    "        \n",
    "        char_hat, hidden = decoder(in_char, hidden)\n",
    "        target_char = target_char.unsqueeze(0)\n",
    "        loss += criterion(char_hat.squeeze(0), target_char)\n",
    "    \n",
    "        i += 1\n",
    "        \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "        \n",
    "    return loss.item() / len(input_string)\n",
    "        \n",
    "def char_tensor(chars):\n",
    "    \"\"\"Converts characters in a string to a numerical index representing that character.\n",
    "    Args:\n",
    "        chars (str): the string to convert\n",
    "    Returns:\n",
    "        tensor (torch.Tensor): a tensor containing the indices of each letter in the string\n",
    "    \"\"\"\n",
    "    all_characters = string.printable    \n",
    "    tensor = torch.zeros(len(chars)).long()\n",
    "    for c in range(len(chars)):                \n",
    "        tensor[c] = all_characters.index(chars[c])\n",
    "    return tensor\n",
    "\n",
    "def evaluate(decoder, prime_str='A', predict_len=100, temperature=0.8):\n",
    "    \"\"\"Samples from the trained RNN\n",
    "    Args:\n",
    "        decoder (torch.nn.Module): the RNN\n",
    "        prime_str (str): a string to prime the model\n",
    "        predict_len (int): the number of characters to predict\n",
    "        temperature (float in [0, 1]): the amount of randomness in the sample\n",
    "    Returns:\n",
    "        predicted (str): a predicted string\n",
    "    \"\"\"\n",
    "    hidden = decoder.init_hidden()\n",
    "    predicted = prime_str\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    all_characters = string.printable  \n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "\n",
    "    for p in range(predict_len):\n",
    "\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "\n",
    "        char_choice = all_characters[top_i]\n",
    "        inp = char_tensor(char_choice)\n",
    "        predicted += char_choice\n",
    "\n",
    "    return predicted \n",
    "\n",
    "def train(n_epochs=5000, print_every=500, plot_every=10, hidden_size=100, n_layers=1,\n",
    "          lr=0.005, file_name=\"michael-jackson.txt\"):\n",
    "    \"\"\"Trains a RNN according to the given parameters\n",
    "    Args:\n",
    "        n_epochs (int): the number of epochs to train for\n",
    "        print_every (int): how often to print an evaluation string\n",
    "        plot_every (int): how often to track the training loss\n",
    "        hidden_size (int): the hidden dimension of the recurrent unit\n",
    "        n_layers (int): the number of layers in the model\n",
    "        lr (float): the learning rate for the model\n",
    "        file_name (str): the name of the file to train on. Options: ['al-green.txt', 'Kanye_West.txt', \n",
    "            'britney-spears.txt', 'kanye-west.txt', 'notorious-big.txt', 'patti-smith.txt', 'prince.txt', \n",
    "            'leonard-cohen.txt', 'dolly-parton.txt', 'janisjoplin.txt', 'amy-winehouse.txt', 'dr-seuss.txt', \n",
    "            'rihanna.txt', 'adele.txt', 'eminem.txt', 'bjork.txt', 'radiohead.txt', 'missy-elliott.txt', \n",
    "            'beatles.txt', 'bruce-springsteen.txt', 'Lil_Wayne.txt', 'nickelback.txt', 'blink-182.txt', \n",
    "            'drake.txt', 'joni-mitchell.txt', 'bob-marley.txt', 'nicki-minaj.txt', 'lady-gaga.txt', \n",
    "            'kanye.txt', 'lorde.txt', 'bob-dylan.txt', 'lil-wayne.txt', 'dickinson.txt', 'bruno-mars.txt', \n",
    "            'alicia-keys.txt', 'r-kelly.txt', 'ludacris.txt', 'bieber.txt', 'nursery_rhymes.txt', \n",
    "            'michael-jackson.txt', 'dj-khaled.txt', 'lin-manuel-miranda.txt', 'paul-simon.txt', 'cake.txt', \n",
    "            'johnny-cash.txt', 'notorious_big.txt', 'nirvana.txt', 'jimi-hendrix.txt', 'disney.txt']\n",
    "    \"\"\"    \n",
    "    all_characters = string.printable\n",
    "    in_size=len(all_characters)\n",
    "    output_size = len(all_characters)\n",
    "    \n",
    "    train_dataset = TextDataset(file_name=file_name)\n",
    "    \n",
    "    decoder = RNN(in_size, hidden_size, output_size, n_layers=n_layers)\n",
    "    \n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    all_losses = []\n",
    "    running_loss = 0\n",
    "    start = time.time()\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        \n",
    "        input_string, target_string = train_dataset.segment_extractor.random_training_set()\n",
    "        loss_ = step(input_string, target_string, decoder, decoder_optimizer, criterion)\n",
    "        running_loss += loss_\n",
    "        \n",
    "        if epoch % print_every == 0:\n",
    "            print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
    "            print(evaluate(decoder, 'Wh', 100), '\\n')\n",
    "            \n",
    "        if epoch % plot_every == 0:\n",
    "            all_losses.append(running_loss / (epoch + 1))\n",
    "            \n",
    "    plt.plot(range(len(all_losses)), all_losses, label='Loss')\n",
    "    plt.xlabel(\"Epoch / {}\".format(plot_every))\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0iXw1bS2gax"
   },
   "source": [
    "# Results - Alexander Hamilton is not throwing away his shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1623687744140625 (0 0%) 4.5989]\n",
      "WhX8x~lI)x+P9\n",
      "4hJw0Y7fVh\toj:_PJssEI`\\3Hdw\u000b",
      "(+kRG#Gf`CN-8 }mv}\u000b",
      "v e_?$&UJWb}o8x}9Wuh)RS0~VG\"^fZ\n",
      "at?o1*:[c \n",
      "\n",
      "[81.7245922088623 (500 5%) 1.9224]\n",
      "What's sint\n",
      "I'm and me pitent a rest withe alto never and sent for in anothesina coleplains and I deak \n",
      "\n",
      "[161.752427816391 (1000 10%) 0.9795]\n",
      "Where's have you shot to have this will have you wan the sad\n",
      "Will the were What the could have we deat \n",
      "\n",
      "[241.7951946258545 (1500 15%) 1.4527]\n",
      "Who me\n",
      "I'm wait for bot hand me who are mys many many and and moshine, but Aled|ary sholdrops in the m \n",
      "\n",
      "[322.07968401908875 (2000 20%) 1.3755]\n",
      "Whot, I do I make regoals on Secret whoul\n",
      "Ham low in parteed in for it\n",
      "Wait for it\n",
      "Wait for the rose c \n",
      "\n",
      "[399.7305579185486 (2500 25%) 1.6160]\n",
      "When America got the sain\n",
      "Wondent to milly will not words ears the bed Ale, sir, of a back is my coall \n",
      "\n",
      "[477.75779724121094 (3000 30%) 1.3693]\n",
      "When dead of colonectress And if we reary\n",
      "One with somern, are been on wis so pirtuat\n",
      "A corse I'm what \n",
      "\n",
      "[556.7428724765778 (3500 35%) 1.1407]\n",
      "Whot wastorerish\n",
      "I peacove a set mead my?\n",
      "If who reddolish and I gonna does Hamide, and something cham \n",
      "\n",
      "[635.1316874027252 (4000 40%) 1.5008]\n",
      "Whoa theer stantles 6one?\n",
      "I have place in pated event deadin' blurry mime, I'm kin' that's desul, deak \n",
      "\n",
      "[714.5842235088348 (4500 45%) 1.1746]\n",
      "Where the meen, do greauly 'emhmolrould prought of a terame on shot\n",
      "I am not throwing awary the consit \n",
      "\n",
      "[793.4432375431061 (5000 50%) 1.1710]\n",
      "Whoa, whoa tally that the manablem is e%rote I mary seeterenster\n",
      "He was you can be a cretugisten's esc \n",
      "\n",
      "[872.46764087677 (5500 55%) 0.2167]\n",
      "When shot I am not throwing away my shot I am not from 'em You and me\n",
      "You'r situated you were mine Lif \n",
      "\n",
      "[951.0537295341492 (6000 60%) 1.1750]\n",
      "When she talk with for theide, for every to his guarre asts for it\n",
      "Wait for it\n",
      "Wait, when so he takes  \n",
      "\n",
      "[1030.182326555252 (6500 65%) 1.1164]\n",
      "What I don't stongless land\n",
      "In AleZ.t Number him inered are you a said\n",
      "When I my step the revolutionar \n",
      "\n",
      "[1110.363249540329 (7000 70%) 0.9363]\n",
      "When she the street them it I'm will never me the histor is to stuth I'm signt time popped I do I got  \n",
      "\n",
      "[1189.8422889709473 (7500 75%) 1.2990]\n",
      "When you our went a predce they dock their \taren't in the get your said of a tran\n",
      "Anims, their mitit\n",
      "W \n",
      "\n",
      "[1270.9717156887054 (8000 80%) 0.2600]\n",
      "While, live me, show the same a peess on so one to have the rovs diep that he's guard an hore Wait for \n",
      "\n",
      "[1352.0412962436676 (8500 85%) 1.1917]\n",
      "Where signt-ch, it's him it a peet of One what we got all your defar\n",
      "Scation an the %oth a plessing in \n",
      "\n",
      "[1432.3828220367432 (9000 90%) 0.2919]\n",
      "Whoed into of a wing 'n and bon we are me you and he know yo eyes and do gonna reming I'm gottlay do I \n",
      "\n",
      "[1511.2165713310242 (9500 95%) 1.3001]\n",
      "Where's more Hamiding stand\n",
      "Sever it the mades, comin' in the shot\n",
      "And I am not throwing away my shoul \n",
      "\n",
      "[1589.861568927765 (10000 100%) 1.6364]\n",
      "Whe pop this way to pome without for doubly\n",
      "Send your patica from to see wrote to take as monee We say \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(n_epochs=10000, file_name='lin-manuel-miranda.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9gj3UWCw2J6q",
    "D-0hC1GV14Gy",
    "IyYLpLrZ2Q0e",
    "dVOONUQJKW3j",
    "evbm5yBq2Ycf",
    "A0iXw1bS2gax"
   ],
   "name": "Lab6CharRNN",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
